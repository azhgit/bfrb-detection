{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51699680",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROJECT COMPLETION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = f\"\"\"\n",
    "\n",
    "Training results:\n",
    "‚îú‚îÄ FFT-MLP: {fft_f1:.4f} F1 ({fft_f1*100:.2f}%)\n",
    "‚îú‚îÄ CNN-BiLSTM: {cnn_f1:.4f} F1 ({cnn_f1*100:.2f}%)\n",
    "‚îî‚îÄ Late Fusion: {fusion_f1:.4f} F1 ({fusion_f1*100:.2f}%)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8499d870",
   "metadata": {},
   "source": [
    "## Project Completion Summary\n",
    "\n",
    "This final cell prints a concise summary of the evaluation results, highlighting key F1 scores for each model.\n",
    "\n",
    "Use this summary as a quick inspection of final performance. For publication or formal reporting, refer to the saved CSV and visualization files created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6293a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare data\n",
    "models = ['FFT-MLP', 'CNN-BiLSTM', 'Late Fusion']\n",
    "f1_scores = [fft_f1, cnn_f1, fusion_f1]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "# Create 6 subplots\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Plot 1: F1 scores comparison\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "bars = ax1.bar(models, f1_scores, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax1.set_ylabel('F1 Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('F1 Scores of the three models', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylim([0.75, 1.0])\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, score in zip(bars, f1_scores):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{score:.4f}\\\\n({score*100:.2f}%)',\n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 2: Accuracy comparison\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "accuracies = [fft_acc, cnn_acc, fusion_acc]\n",
    "bars = ax2.bar(models, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax2.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Accuracy comparison', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylim([0.75, 1.0])\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{acc:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 3: Late Fusion confusion matrix\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "sns.heatmap(fusion_cm, annot=True, fmt='d', cmap='Blues', ax=ax3, cbar=False)\n",
    "ax3.set_title('Late Fusion Confusion Matrix', fontsize=13, fontweight='bold')\n",
    "ax3.set_ylabel('True Label')\n",
    "ax3.set_xlabel('Predicted Label')\n",
    "\n",
    "# Plot 4: All metrics comparison\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "metrics = ['F1', 'Accuracy', 'Precision', 'Recall']\n",
    "fft_metrics = [fft_f1, fft_acc, fft_precision, fft_recall]\n",
    "cnn_metrics = [cnn_f1, cnn_acc, cnn_precision, cnn_recall]\n",
    "fusion_metrics = [fusion_f1, fusion_acc, fusion_precision, fusion_recall]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "ax4.bar(x - width, fft_metrics, width, label='FFT-MLP', color='#FF6B6B', alpha=0.8)\n",
    "ax4.bar(x, cnn_metrics, width, label='CNN-BiLSTM', color='#4ECDC4', alpha=0.8)\n",
    "ax4.bar(x + width, fusion_metrics, width, label='Late Fusion', color='#45B7D1', alpha=0.8)\n",
    "\n",
    "ax4.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax4.set_title('All Metrics Comparison', fontsize=13, fontweight='bold')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(metrics)\n",
    "ax4.legend()\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "ax4.set_ylim([0.75, 1.0])\n",
    "\n",
    "# Plot 5: Performance ranking\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "sorted_indices = np.argsort(f1_scores)[::-1]\n",
    "sorted_models = [models[i] for i in sorted_indices]\n",
    "sorted_scores = [f1_scores[i] for i in sorted_indices]\n",
    "sorted_colors = [colors[i] for i in sorted_indices]\n",
    "\n",
    "bars = ax5.barh(sorted_models, sorted_scores, color=sorted_colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax5.set_xlabel('F1 Score', fontsize=12, fontweight='bold')\n",
    "ax5.set_title('Model performance ranking', fontsize=13, fontweight='bold')\n",
    "ax5.set_xlim([0.75, 1.0])\n",
    "ax5.grid(axis='x', alpha=0.3)\n",
    "\n",
    "for i, (bar, score) in enumerate(zip(bars, sorted_scores)):\n",
    "    rank = 'ü•á' if i == 0 else 'ü•à' if i == 1 else 'ü•â'\n",
    "    ax5.text(score - 0.02, bar.get_y() + bar.get_height()/2.,\n",
    "             f'{rank} {score:.4f}',\n",
    "             ha='right', va='center', fontweight='bold', color='white', fontsize=11)\n",
    "\n",
    "# Plot 6: Metrics table\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "ax6.axis('tight')\n",
    "ax6.axis('off')\n",
    "\n",
    "table_data = [\n",
    "    ['FFT-MLP', f'{fft_f1:.4f}', f'{fft_acc:.4f}', f'{fft_precision:.4f}', f'{fft_recall:.4f}'],\n",
    "    ['CNN-BiLSTM', f'{cnn_f1:.4f}', f'{cnn_acc:.4f}', f'{cnn_precision:.4f}', f'{cnn_recall:.4f}'],\n",
    "    ['Late Fusion', f'{fusion_f1:.4f}', f'{fusion_acc:.4f}', f'{fusion_precision:.4f}', f'{fusion_recall:.4f}']\n",
    "]\n",
    "\n",
    "table = ax6.table(cellText=table_data,\n",
    "                 colLabels=['Model', 'F1', 'Accuracy', 'Precision', 'Recall'],\n",
    "                 cellLoc='center', loc='center')\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "for i in range(5):\n",
    "    table[(0, i)].set_facecolor('#4ECDC4')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "for i in range(1, 4):\n",
    "    for j in range(5):\n",
    "        if i % 2 == 0:\n",
    "            table[(i, j)].set_facecolor('#f0f0f0')\n",
    "\n",
    "ax6.set_title('Detailed Metrics Table', fontsize=13, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/final_evaluation_visualization.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n‚úÖ Visualization saved to: ../data/final_evaluation_visualization.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0ad5fb",
   "metadata": {},
   "source": [
    "## Visualization Components and Output\n",
    "\n",
    "This cell produces a multi-panel figure summarizing model performance for reporting and publication.\n",
    "\n",
    "Figure composition:\n",
    "\n",
    "- Panel 1: Bar chart comparing F1 scores across models (explicit values shown).\n",
    "- Panel 2: Accuracy comparison bar chart.\n",
    "- Panel 3: Confusion matrix (Late Fusion).\n",
    "- Panel 4: Grouped bar chart comparing F1 / Accuracy / Precision / Recall.\n",
    "- Panel 5: Horizontal ranked bar chart (performance ordering with medals).\n",
    "- Panel 6: Detailed metrics table.\n",
    "\n",
    "Implementation notes:\n",
    "\n",
    "- Visual styles use `seaborn` with `whitegrid` and consistent color palette.\n",
    "- Y-axis ranges are chosen for clarity (e.g., 0.75‚Äì1.0).\n",
    "- The final figure is saved to `../data/final_evaluation_visualization.png` at 300 DPI for inclusion in reports or publications.\n",
    "\n",
    "Export: The high-resolution PNG and the CSV output together provide both visual and tabular artifacts for downstream use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03cdf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_data = {\n",
    "    'Model': ['FFT-MLP', 'CNN-BiLSTM', 'Late Fusion'],\n",
    "    'F1 Score': [fft_f1, cnn_f1, fusion_f1],\n",
    "    'Accuracy': [fft_acc, cnn_acc, fusion_acc],\n",
    "    'Precision': [fft_precision, cnn_precision, fusion_precision],\n",
    "    'Recall': [fft_recall, cnn_recall, fusion_recall]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "print(\"\\nDetailed metrics:\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('../data/final_results.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"\\n‚úÖ Results saved to: ../data/final_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339fc2d4",
   "metadata": {},
   "source": [
    "## Results Aggregation and Export\n",
    "\n",
    "This cell consolidates per-model metrics into a `pandas.DataFrame` and persists the results to CSV for reproducibility and external analysis.\n",
    "\n",
    "Actions taken:\n",
    "\n",
    "- Assemble a tabular summary with columns: `Model`, `F1 Score`, `Accuracy`, `Precision`, `Recall`.\n",
    "- Print a human-readable table to the notebook output for quick inspection.\n",
    "- Save the results to `../data/final_results.csv` using UTF-8 with BOM (`utf-8-sig`) for Excel compatibility.\n",
    "\n",
    "Tip: Keep this CSV as the canonical machine-readable summary when sharing results or preparing reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab070e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. EVALUATING ALL MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Evaluate FFT-MLP\n",
    "print(\"\\nEvaluating FFT-MLP...\")\n",
    "fft_preds = []\n",
    "fft_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in val_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        logits = fft_mlp_model(X_batch)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        fft_preds.extend(preds.cpu().numpy())\n",
    "        fft_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "fft_f1 = f1_score(fft_labels, fft_preds)\n",
    "fft_acc = accuracy_score(fft_labels, fft_preds)\n",
    "fft_precision = precision_score(fft_labels, fft_preds)\n",
    "fft_recall = recall_score(fft_labels, fft_preds)\n",
    "fft_cm = confusion_matrix(fft_labels, fft_preds)\n",
    "\n",
    "print(f\"‚úÖ FFT-MLP: F1={fft_f1:.4f}, Acc={fft_acc:.4f}, Prec={fft_precision:.4f}, Recall={fft_recall:.4f}\")\n",
    "\n",
    "# Evaluate CNN-BiLSTM\n",
    "print(\"Evaluating CNN-BiLSTM...\")\n",
    "cnn_preds = []\n",
    "cnn_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in val_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        logits = cnn_bilstm_model(X_batch)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        cnn_preds.extend(preds.cpu().numpy())\n",
    "        cnn_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "cnn_f1 = f1_score(cnn_labels, cnn_preds)\n",
    "cnn_acc = accuracy_score(cnn_labels, cnn_preds)\n",
    "cnn_precision = precision_score(cnn_labels, cnn_preds)\n",
    "cnn_recall = recall_score(cnn_labels, cnn_preds)\n",
    "cnn_cm = confusion_matrix(cnn_labels, cnn_preds)\n",
    "\n",
    "print(f\"‚úÖ CNN-BiLSTM: F1={cnn_f1:.4f}, Acc={cnn_acc:.4f}, Prec={cnn_precision:.4f}, Recall={cnn_recall:.4f}\")\n",
    "\n",
    "# Evaluate Late Fusion\n",
    "print(\"Evaluating Late Fusion...\")\n",
    "fusion_preds = []\n",
    "fusion_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in val_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        \n",
    "        # FFT predictions\n",
    "        fft_logits = fft_mlp_model(X_batch)\n",
    "        fft_proba = torch.softmax(fft_logits, dim=1)\n",
    "        \n",
    "        # CNN predictions\n",
    "        cnn_logits = cnn_bilstm_model(X_batch)\n",
    "        cnn_proba = torch.softmax(cnn_logits, dim=1)\n",
    "        \n",
    "        # Fusion\n",
    "        fusion_proba = 0.3 * fft_proba + 0.7 * cnn_proba\n",
    "        preds = torch.argmax(fusion_proba, dim=1)\n",
    "        \n",
    "        fusion_preds.extend(preds.cpu().numpy())\n",
    "        fusion_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "fusion_f1 = f1_score(fusion_labels, fusion_preds)\n",
    "fusion_acc = accuracy_score(fusion_labels, fusion_preds)\n",
    "fusion_precision = precision_score(fusion_labels, fusion_preds)\n",
    "fusion_recall = recall_score(fusion_labels, fusion_preds)\n",
    "fusion_cm = confusion_matrix(fusion_labels, fusion_preds)\n",
    "\n",
    "print(f\"‚úÖ Late Fusion: F1={fusion_f1:.4f}, Acc={fusion_acc:.4f}, Prec={fusion_precision:.4f}, Recall={fusion_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcdaa7e",
   "metadata": {},
   "source": [
    "## Model Evaluation Procedure\n",
    "\n",
    "This section evaluates each model on the validation dataset and computes standard classification metrics.\n",
    "\n",
    "Evaluation workflow:\n",
    "\n",
    "1. Run inference with `torch.no_grad()` to avoid gradient computation and reduce memory usage.\n",
    "2. For each batch, obtain model logits and derive predictions via `argmax`.\n",
    "3. Convert logits to probabilities where needed (softmax) for the late-fusion ensemble.\n",
    "4. Compute metrics for each model:\n",
    "   - F1 score (primary metric)\n",
    "   - Accuracy\n",
    "   - Precision\n",
    "   - Recall\n",
    "   - Confusion matrix\n",
    "\n",
    "Late Fusion:\n",
    "\n",
    "- The late-fusion ensemble combines model probabilities using predefined weights (default 0.3 for FFT-MLP, 0.7 for CNN-BiLSTM).\n",
    "- The fused probabilities are converted to final predictions by `argmax` and evaluated like individual models.\n",
    "\n",
    "All computed metrics and confusion matrices are stored for downstream reporting and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a89dc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. LOADING TRAINED MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load FFT-MLP\n",
    "fft_mlp_model = FFT_MLP(imu_dim=7, hidden_dim=128, num_classes=2).to(device)\n",
    "fft_mlp_model.load_state_dict(torch.load('../data/fft_mlp_best_final.pth'))\n",
    "fft_mlp_model.eval()\n",
    "print(\"\\n‚úÖ FFT-MLP loaded\")\n",
    "\n",
    "# Load CNN-BiLSTM\n",
    "cnn_bilstm_model = CNN_BiLSTM().to(device)\n",
    "cnn_bilstm_model.load_state_dict(torch.load('../data/cnn_bilstm_best.pth'))\n",
    "cnn_bilstm_model.eval()\n",
    "print(\"‚úÖ CNN-BiLSTM loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa67356",
   "metadata": {},
   "source": [
    "## Loading Pre-trained Models\n",
    "\n",
    "This cell loads saved model weights for the two trained architectures and sets them to evaluation mode.\n",
    "\n",
    "Operations:\n",
    "\n",
    "- Instantiate model classes (`FFT_MLP`, `CNN_BiLSTM`) with the same architecture used during training\n",
    "- Load corresponding `.pth` checkpoint files from `../data/`\n",
    "- Call `.eval()` to disable dropout and other training-time behaviors\n",
    "\n",
    "Why this matters:\n",
    "\n",
    "- Ensuring architecture matches the checkpoint prevents state-dict load errors.\n",
    "- Evaluation mode ensures deterministic inference suitable for metric computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941a066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"0. LOADING VALIDATION DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nLoading validation data...\")\n",
    "\n",
    "# Check files exist\n",
    "import os\n",
    "val_files = ['../data/X_val_preprocessed.pkl', '../data/y_val_preprocessed.pkl']\n",
    "for f in val_files:\n",
    "    if os.path.exists(f):\n",
    "        print(f\"‚úÖ Found: {f}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Not found: {f}\")\n",
    "\n",
    "# Load data\n",
    "X_val_preprocessed = pickle.load(open('../data/X_val_preprocessed.pkl', 'rb'))\n",
    "y_val_preprocessed = pickle.load(open('../data/y_val_preprocessed.pkl', 'rb'))\n",
    "\n",
    "# Convert to numpy array (if DataFrame)\n",
    "if isinstance(X_val_preprocessed, pd.DataFrame):\n",
    "    X_val_preprocessed = X_val_preprocessed.values\n",
    "    print(\"‚úÖ X_val converted from DataFrame to numpy array\")\n",
    "\n",
    "if isinstance(y_val_preprocessed, pd.DataFrame):\n",
    "    y_val_preprocessed = y_val_preprocessed.values.flatten()\n",
    "    print(\"‚úÖ y_val converted from DataFrame to numpy array\")\n",
    "elif isinstance(y_val_preprocessed, pd.Series):\n",
    "    y_val_preprocessed = y_val_preprocessed.values\n",
    "    print(\"‚úÖ y_val converted from Series to numpy array\")\n",
    "\n",
    "# Convert to tensors\n",
    "X_val_tensor = torch.FloatTensor(X_val_preprocessed.copy())\n",
    "y_val_tensor = torch.LongTensor(y_val_preprocessed.copy())\n",
    "\n",
    "print(f\"\\n‚úÖ Validation data shape: {X_val_tensor.shape}\")\n",
    "print(f\"‚úÖ Validation labels shape: {y_val_tensor.shape}\")\n",
    "\n",
    "# Create DataLoader\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"‚úÖ DataLoader created (batch_size=128)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985c462a",
   "metadata": {},
   "source": [
    "## Loading Validation Data\n",
    "\n",
    "This cell loads preprocessed validation data from disk and prepares it for evaluation.\n",
    "\n",
    "Steps performed:\n",
    "\n",
    "- Verify the expected pickle files exist under `../data/`\n",
    "- Load `X_val_preprocessed` and `y_val_preprocessed`\n",
    "- Convert pandas objects (DataFrame / Series) to NumPy arrays when necessary\n",
    "- Convert NumPy arrays to PyTorch tensors and create a `DataLoader` for evaluation\n",
    "\n",
    "Notes:\n",
    "\n",
    "- The `DataLoader` is created with `batch_size=128` and `shuffle=False` to ensure deterministic evaluation.\n",
    "- Shapes are printed to help verify data integrity before running the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831f946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import pickle  \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Settings\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ============================================================================\n",
    "# Define model classes (copied from Notebook 4)\n",
    "# ============================================================================\n",
    "\n",
    "# 1. FFT-MLP model\n",
    "class FFT_MLP(nn.Module):\n",
    "    \"\"\"FFT-MLP model\"\"\"\n",
    "    \n",
    "    def __init__(self, imu_dim=7, hidden_dim=128, num_classes=2):\n",
    "        super(FFT_MLP, self).__init__()\n",
    "        \n",
    "        self.imu_dim = imu_dim\n",
    "        \n",
    "        # Number of features after FFT\n",
    "        fft_features = imu_dim * 2  # 14\n",
    "        \n",
    "        # MLP layers\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(fft_features, hidden_dim),  # 14 ‚Üí 128\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(hidden_dim, 64),  # 128 ‚Üí 64\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(64, num_classes)  # 64 ‚Üí 2\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract IMU data\n",
    "        imu_data = x[:, :self.imu_dim]\n",
    "        \n",
    "        # FFT transform\n",
    "        fft_result = torch.fft.fft(imu_data, dim=1)\n",
    "        fft_real = torch.real(fft_result)\n",
    "        fft_imag = torch.imag(fft_result)\n",
    "        fft_features = torch.cat([fft_real, fft_imag], dim=1)\n",
    "        \n",
    "        # MLP classification\n",
    "        output = self.mlp(fft_features)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# 2. CNN-BiLSTM model\n",
    "class CNN_BiLSTM(nn.Module):\n",
    "    \"\"\"CNN-BiLSTM model\"\"\"\n",
    "    \n",
    "    def __init__(self, tof_dim=320, hidden_dim=64, num_classes=2):\n",
    "        super(CNN_BiLSTM, self).__init__()\n",
    "        \n",
    "        self.tof_dim = tof_dim\n",
    "        \n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1024,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        tof_data = x[:, -self.tof_dim:]\n",
    "        batch_size = x.shape[0]\n",
    "        tof_2d = tof_data.view(batch_size, 1, 64, 5)\n",
    "        \n",
    "        x = self.conv1(tof_2d)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        x = x.unsqueeze(1)\n",
    "        x, (h_n, c_n) = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"\\n‚úÖ Model classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7e7818",
   "metadata": {},
   "source": [
    "# Evaluation Notebook\n",
    "\n",
    "This notebook performs final evaluation and reporting for the BFRB detection models. It is organized to:\n",
    "\n",
    "- Load preprocessed validation data\n",
    "- Load pre-trained model weights\n",
    "- Evaluate `FFT-MLP`, `CNN-BiLSTM`, and a late-fusion ensemble\n",
    "- Aggregate metrics and save results to CSV\n",
    "- Generate publication-quality visualizations and a concise project summary\n",
    "\n",
    "The code cells below initialize the environment (imports, device selection, plotting style) and define the model classes used for inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
