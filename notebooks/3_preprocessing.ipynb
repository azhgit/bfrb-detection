{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b437188a",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "\n",
    "This cell loads the raw CSV files into pandas DataFrames.\n",
    "- Inputs: `../data/train.csv`, `../data/test.csv`\n",
    "- Outputs: `train_df`, `test_df`\n",
    "- Purpose: quickly inspect shapes to confirm files were read correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620cb180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "print('Original shapes:')\n",
    "print(f'Train: {train_df.shape}')\n",
    "print(f'Test: {test_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3775230e",
   "metadata": {},
   "source": [
    "# Column comparison\n",
    "\n",
    "Compare columns between `train_df` and `test_df` to identify mismatches and common features.\n",
    "This helps align feature sets before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8816f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*80)\n",
    "print('COLUMN COMPARISON')\n",
    "print('='*80)\n",
    "\n",
    "print(f'\\nTrain columns: {len(train_df.columns)}')\n",
    "print(f'Test columns: {len(test_df.columns)}')\n",
    "\n",
    "# Find columns present in train but not in test\n",
    "missing_in_test = set(train_df.columns) - set(test_df.columns)\n",
    "print(f'\\nColumns in TRAIN but NOT in TEST: {missing_in_test}')\n",
    "\n",
    "# Find common columns\n",
    "common_cols = set(train_df.columns) & set(test_df.columns)\n",
    "print(f'Common columns: {len(common_cols)}')\n",
    "\n",
    "# Print all test columns\n",
    "print('\\nAll TEST columns:')\n",
    "print(test_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5af65a2",
   "metadata": {},
   "source": [
    "# Feature selection and splitting\n",
    "\n",
    "Define the label column and select feature columns by excluding metadata columns. Create `X_train`, `y_train` and align `X_test` with available test features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3307f2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label column\n",
    "label_col = 'sequence_type'\n",
    "\n",
    "# Feature columns (exclude metadata)\n",
    "metadata_cols = ['row_id', 'sequence_id', 'sequence_counter', 'subject', \n",
    "                 'orientation', label_col]\n",
    "feature_cols = [col for col in train_df.columns if col not in metadata_cols]\n",
    "\n",
    "print(f'Feature columns: {len(feature_cols)}')\n",
    "print(f'Label column: {label_col}')\n",
    "\n",
    "# Split features and label\n",
    "X_train = train_df[feature_cols].copy()\n",
    "y_train = train_df[label_col].copy()\n",
    "\n",
    "# Important: for test_df, only select columns that exist in test_df\n",
    "feature_cols_test = [col for col in feature_cols if col in test_df.columns]\n",
    "\n",
    "X_test = test_df[feature_cols_test].copy()\n",
    "\n",
    "print(f'\\nX_train shape: {X_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "\n",
    "print(f'\\nColumns in train but not test: {set(feature_cols) - set(feature_cols_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1903ed4",
   "metadata": {},
   "source": [
    "# Missing value handling\n",
    "\n",
    "Inspect missing values and apply imputation:\n",
    "- Drop columns with >50% missing values.\n",
    "- Fill numeric columns using the training median.\n",
    "- Fill categorical/string columns using the training mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45163b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*80)\n",
    "print('HANDLING MISSING VALUES')\n",
    "print('='*80)\n",
    "\n",
    "# Check missing values (only common columns)\n",
    "common_feature_cols = [col for col in X_train.columns if col in X_test.columns]\n",
    "\n",
    "print(f'\\nMissing values in X_train:')\n",
    "missing_counts = X_train[common_feature_cols].isnull().sum()\n",
    "missing_pct = (missing_counts / len(X_train)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_counts,\n",
    "    'Missing_Percentage': missing_pct\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "print(missing_df.head(20))\n",
    "\n",
    "# Strategy: drop columns with >50% missing values\n",
    "cols_to_drop = missing_df[missing_df['Missing_Percentage'] > 50].index.tolist()\n",
    "print(f'\\nColumns to drop (>50% missing): {len(cols_to_drop)}')\n",
    "\n",
    "# Drop these columns\n",
    "X_train = X_train.drop(columns=cols_to_drop)\n",
    "X_test = X_test.drop(columns=[col for col in cols_to_drop if col in X_test.columns])\n",
    "\n",
    "# Fill remaining missing values: numeric -> median, string/object -> mode\n",
    "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "string_cols = X_train.select_dtypes(include=['str', 'object']).columns.tolist()\n",
    "\n",
    "print(f'\\nNumeric columns: {len(numeric_cols)}')\n",
    "print(f'String columns: {len(string_cols)}')\n",
    "if string_cols:\n",
    "    print(f'String column examples: {string_cols[:5]}')\n",
    "\n",
    "# Compute medians on training numeric columns\n",
    "train_medians = X_train[numeric_cols].median()\n",
    "\n",
    "# Fill numeric columns with medians\n",
    "X_train[numeric_cols] = X_train[numeric_cols].fillna(train_medians)\n",
    "for col in numeric_cols:\n",
    "    if col in X_test.columns:\n",
    "        X_test[col] = X_test[col].fillna(train_medians[col])\n",
    "\n",
    "# Fill string/object columns with mode\n",
    "for col in string_cols:\n",
    "    mode_value = X_train[col].mode()[0]\n",
    "    X_train[col] = X_train[col].fillna(mode_value)\n",
    "    if col in X_test.columns:\n",
    "        X_test[col] = X_test[col].fillna(mode_value)\n",
    "\n",
    "print(f'\\nAfter handling missing values:')\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "print(f'Remaining missing values in X_train: {X_train.isnull().sum().sum()}')\n",
    "print(f'Remaining missing values in X_test: {X_test.isnull().sum().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baa4168",
   "metadata": {},
   "source": [
    "# Encode target variable\n",
    "\n",
    "Map categorical target labels to numeric values for modeling.\n",
    "- Example mapping: `Target` -> 1, `Non-Target` -> 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d16831",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*80)\n",
    "print('ENCODING TARGET VARIABLE')\n",
    "print('='*80)\n",
    "\n",
    "# View label distribution\n",
    "print('\\nOriginal label distribution:')\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Encode: Target=1, Non-Target=0\n",
    "label_mapping = {'Target': 1, 'Non-Target': 0}\n",
    "y_train_encoded = y_train.map(label_mapping)\n",
    "\n",
    "print('\\nEncoded label distribution:')\n",
    "print(y_train_encoded.value_counts())\n",
    "print(f'\\nLabel mapping: {label_mapping}')\n",
    "\n",
    "y_train = y_train_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da88a25",
   "metadata": {},
   "source": [
    "# Train / Validation split\n",
    "\n",
    "Split the processed training data into training and validation sets (80/20) while preserving class proportions using stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afef12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*80)\n",
    "print('TRAIN-VALIDATION SPLIT')\n",
    "print('='*80)\n",
    "\n",
    "# 80-20 split\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_train  # preserve class proportions\n",
    ")\n",
    "\n",
    "print(f'\\nTrain split: {X_train_split.shape}')\n",
    "print(f'Validation split: {X_val.shape}')\n",
    "print(f'Test: {X_test.shape}')\n",
    "\n",
    "print(f'\\nClass distribution in train split:')\n",
    "print(y_train_split.value_counts())\n",
    "print(f'\\nClass distribution in validation split:')\n",
    "print(y_val.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4758654d",
   "metadata": {},
   "source": [
    "# Remove non-numeric columns\n",
    "\n",
    "Detect and remove `object`/string columns from each split to ensure only numeric data is scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec53cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure removal of any string/object columns\n",
    "print('Checking for string columns...')\n",
    "\n",
    "# Remove string/object columns from X_train_split\n",
    "string_cols_train = X_train_split.select_dtypes(include=['str','object']).columns.tolist()\n",
    "if string_cols_train:\n",
    "    print(f'Removing {len(string_cols_train)} string columns from X_train_split: {string_cols_train}')\n",
    "    X_train_split = X_train_split.drop(columns=string_cols_train)\n",
    "\n",
    "# Remove string/object columns from X_val\n",
    "string_cols_val = X_val.select_dtypes(include=['str','object']).columns.tolist()\n",
    "if string_cols_val:\n",
    "    print(f'Removing {len(string_cols_val)} string columns from X_val: {string_cols_val}')\n",
    "    X_val = X_val.drop(columns=string_cols_val)\n",
    "\n",
    "# Remove string/object columns from X_test\n",
    "string_cols_test = X_test.select_dtypes(include=['str','object']).columns.tolist()\n",
    "if string_cols_test:\n",
    "    print(f'Removing {len(string_cols_test)} string columns from X_test: {string_cols_test}')\n",
    "    X_test = X_test.drop(columns=string_cols_test)\n",
    "\n",
    "print(f'\\nFinal shapes:')\n",
    "print(f'X_train_split: {X_train_split.shape} - dtypes: {X_train_split.dtypes.value_counts().to_dict()}')\n",
    "print(f'X_val: {X_val.shape} - dtypes: {X_val.dtypes.value_counts().to_dict()}')\n",
    "print(f'X_test: {X_test.shape} - dtypes: {X_test.dtypes.value_counts().to_dict()}')\n",
    "\n",
    "print('\\n Ready for scaling!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f385a734",
   "metadata": {},
   "source": [
    "# Standardization\n",
    "\n",
    "Fit a `StandardScaler` on the training split and apply it to validation and test sets. Convert results back to DataFrames for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ddd41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*80)\n",
    "print('STANDARDIZATION')\n",
    "print('='*80)\n",
    "\n",
    "# Fit scaler on train split\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_split)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print('Scaler fitted on train split')\n",
    "print(f'Mean of scaled train: {X_train_scaled.mean():.6f}')\n",
    "print(f'Std of scaled train: {X_train_scaled.std():.6f}')\n",
    "\n",
    "# Convert back to DataFrame\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train_split.columns)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "print('\\nScaled shapes:')\n",
    "print(f'X_train_scaled: {X_train_scaled.shape}')\n",
    "print(f'X_val_scaled: {X_val_scaled.shape}')\n",
    "print(f'X_test_scaled: {X_test_scaled.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d79944",
   "metadata": {},
   "source": [
    "# Save preprocessed data\n",
    "\n",
    "Persist the preprocessed datasets and the fitted scaler to `../data/` using pickle for downstream modeling and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aab1b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "print('='*80)\n",
    "print('SAVING PREPROCESSED DATA')\n",
    "print('='*80)\n",
    "\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "# Save as pickle (preserve column info)\n",
    "with open('../data/X_train_preprocessed.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train_scaled, f)\n",
    "\n",
    "with open('../data/X_val_preprocessed.pkl', 'wb') as f:\n",
    "    pickle.dump(X_val_scaled, f)\n",
    "\n",
    "with open('../data/X_test_preprocessed.pkl', 'wb') as f:\n",
    "    pickle.dump(X_test_scaled, f)\n",
    "\n",
    "with open('../data/y_train_preprocessed.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train_split, f)\n",
    "\n",
    "with open('../data/y_val_preprocessed.pkl', 'wb') as f:\n",
    "    pickle.dump(y_val, f)\n",
    "\n",
    "# Also save the scaler (for later inference)\n",
    "with open('../data/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print('  Preprocessed data saved:')\n",
    "print('  - X_train_preprocessed.pkl')\n",
    "print('  - X_val_preprocessed.pkl')\n",
    "print('  - X_test_preprocessed.pkl')\n",
    "print('  - y_train_preprocessed.pkl')\n",
    "print('  - y_val_preprocessed.pkl')\n",
    "print('  - scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a3dea7",
   "metadata": {},
   "source": [
    "# Preprocessing summary\n",
    "\n",
    "Summarize preprocessing results (rows, dropped columns, final feature count, split sizes, and class counts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61659544",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*80)\n",
    "print('PREPROCESSING SUMMARY')\n",
    "print('='*80)\n",
    "\n",
    "summary = {\n",
    "    'Original Train Rows': len(train_df),\n",
    "    'Columns Dropped': len(cols_to_drop),\n",
    "    'Final Features': X_train_scaled.shape[1],\n",
    "    'Train Split': X_train_scaled.shape[0],\n",
    "    'Validation Split': X_val_scaled.shape[0],\n",
    "    'Test': X_test_scaled.shape[0],\n",
    "    'Target=1': (y_train_split == 1).sum(),\n",
    "    'Target=0': (y_train_split == 0).sum(),\n",
    "}\n",
    "\n",
    "for key, value in summary.items():\n",
    "    print(f'{key}: {value}')\n",
    "\n",
    "print('\\n Preprocessing Complete!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
