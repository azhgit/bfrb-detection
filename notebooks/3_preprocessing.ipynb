{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b437188a",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "\n",
    "This cell loads the raw CSV files into pandas DataFrames.\n",
    "- Inputs: `../data/train.csv`, `../data/test.csv`\n",
    "- Outputs: `train_df`, `test_df`\n",
    "- Purpose: quickly inspect shapes to confirm files were read correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620cb180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shapes:\n",
      "Train: (574945, 341)\n",
      "Test: (107, 336)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "print('Original shapes:')\n",
    "print(f'Train: {train_df.shape}')\n",
    "print(f'Test: {test_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3775230e",
   "metadata": {},
   "source": [
    "# Column comparison\n",
    "\n",
    "Compare columns between `train_df` and `test_df` to identify mismatches and common features.\n",
    "This helps align feature sets before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8816f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COLUMN COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Train columns: 341\n",
      "Test columns: 336\n",
      "\n",
      "Columns in TRAIN but NOT in TEST: {'orientation', 'gesture', 'behavior', 'sequence_type', 'phase'}\n",
      "Common columns: 336\n",
      "\n",
      "All TEST columns:\n",
      "['row_id', 'sequence_id', 'sequence_counter', 'subject', 'acc_x', 'acc_y', 'acc_z', 'rot_w', 'rot_x', 'rot_y', 'rot_z', 'thm_1', 'thm_2', 'thm_3', 'thm_4', 'thm_5', 'tof_1_v0', 'tof_1_v1', 'tof_1_v2', 'tof_1_v3', 'tof_1_v4', 'tof_1_v5', 'tof_1_v6', 'tof_1_v7', 'tof_1_v8', 'tof_1_v9', 'tof_1_v10', 'tof_1_v11', 'tof_1_v12', 'tof_1_v13', 'tof_1_v14', 'tof_1_v15', 'tof_1_v16', 'tof_1_v17', 'tof_1_v18', 'tof_1_v19', 'tof_1_v20', 'tof_1_v21', 'tof_1_v22', 'tof_1_v23', 'tof_1_v24', 'tof_1_v25', 'tof_1_v26', 'tof_1_v27', 'tof_1_v28', 'tof_1_v29', 'tof_1_v30', 'tof_1_v31', 'tof_1_v32', 'tof_1_v33', 'tof_1_v34', 'tof_1_v35', 'tof_1_v36', 'tof_1_v37', 'tof_1_v38', 'tof_1_v39', 'tof_1_v40', 'tof_1_v41', 'tof_1_v42', 'tof_1_v43', 'tof_1_v44', 'tof_1_v45', 'tof_1_v46', 'tof_1_v47', 'tof_1_v48', 'tof_1_v49', 'tof_1_v50', 'tof_1_v51', 'tof_1_v52', 'tof_1_v53', 'tof_1_v54', 'tof_1_v55', 'tof_1_v56', 'tof_1_v57', 'tof_1_v58', 'tof_1_v59', 'tof_1_v60', 'tof_1_v61', 'tof_1_v62', 'tof_1_v63', 'tof_2_v0', 'tof_2_v1', 'tof_2_v2', 'tof_2_v3', 'tof_2_v4', 'tof_2_v5', 'tof_2_v6', 'tof_2_v7', 'tof_2_v8', 'tof_2_v9', 'tof_2_v10', 'tof_2_v11', 'tof_2_v12', 'tof_2_v13', 'tof_2_v14', 'tof_2_v15', 'tof_2_v16', 'tof_2_v17', 'tof_2_v18', 'tof_2_v19', 'tof_2_v20', 'tof_2_v21', 'tof_2_v22', 'tof_2_v23', 'tof_2_v24', 'tof_2_v25', 'tof_2_v26', 'tof_2_v27', 'tof_2_v28', 'tof_2_v29', 'tof_2_v30', 'tof_2_v31', 'tof_2_v32', 'tof_2_v33', 'tof_2_v34', 'tof_2_v35', 'tof_2_v36', 'tof_2_v37', 'tof_2_v38', 'tof_2_v39', 'tof_2_v40', 'tof_2_v41', 'tof_2_v42', 'tof_2_v43', 'tof_2_v44', 'tof_2_v45', 'tof_2_v46', 'tof_2_v47', 'tof_2_v48', 'tof_2_v49', 'tof_2_v50', 'tof_2_v51', 'tof_2_v52', 'tof_2_v53', 'tof_2_v54', 'tof_2_v55', 'tof_2_v56', 'tof_2_v57', 'tof_2_v58', 'tof_2_v59', 'tof_2_v60', 'tof_2_v61', 'tof_2_v62', 'tof_2_v63', 'tof_3_v0', 'tof_3_v1', 'tof_3_v2', 'tof_3_v3', 'tof_3_v4', 'tof_3_v5', 'tof_3_v6', 'tof_3_v7', 'tof_3_v8', 'tof_3_v9', 'tof_3_v10', 'tof_3_v11', 'tof_3_v12', 'tof_3_v13', 'tof_3_v14', 'tof_3_v15', 'tof_3_v16', 'tof_3_v17', 'tof_3_v18', 'tof_3_v19', 'tof_3_v20', 'tof_3_v21', 'tof_3_v22', 'tof_3_v23', 'tof_3_v24', 'tof_3_v25', 'tof_3_v26', 'tof_3_v27', 'tof_3_v28', 'tof_3_v29', 'tof_3_v30', 'tof_3_v31', 'tof_3_v32', 'tof_3_v33', 'tof_3_v34', 'tof_3_v35', 'tof_3_v36', 'tof_3_v37', 'tof_3_v38', 'tof_3_v39', 'tof_3_v40', 'tof_3_v41', 'tof_3_v42', 'tof_3_v43', 'tof_3_v44', 'tof_3_v45', 'tof_3_v46', 'tof_3_v47', 'tof_3_v48', 'tof_3_v49', 'tof_3_v50', 'tof_3_v51', 'tof_3_v52', 'tof_3_v53', 'tof_3_v54', 'tof_3_v55', 'tof_3_v56', 'tof_3_v57', 'tof_3_v58', 'tof_3_v59', 'tof_3_v60', 'tof_3_v61', 'tof_3_v62', 'tof_3_v63', 'tof_4_v0', 'tof_4_v1', 'tof_4_v2', 'tof_4_v3', 'tof_4_v4', 'tof_4_v5', 'tof_4_v6', 'tof_4_v7', 'tof_4_v8', 'tof_4_v9', 'tof_4_v10', 'tof_4_v11', 'tof_4_v12', 'tof_4_v13', 'tof_4_v14', 'tof_4_v15', 'tof_4_v16', 'tof_4_v17', 'tof_4_v18', 'tof_4_v19', 'tof_4_v20', 'tof_4_v21', 'tof_4_v22', 'tof_4_v23', 'tof_4_v24', 'tof_4_v25', 'tof_4_v26', 'tof_4_v27', 'tof_4_v28', 'tof_4_v29', 'tof_4_v30', 'tof_4_v31', 'tof_4_v32', 'tof_4_v33', 'tof_4_v34', 'tof_4_v35', 'tof_4_v36', 'tof_4_v37', 'tof_4_v38', 'tof_4_v39', 'tof_4_v40', 'tof_4_v41', 'tof_4_v42', 'tof_4_v43', 'tof_4_v44', 'tof_4_v45', 'tof_4_v46', 'tof_4_v47', 'tof_4_v48', 'tof_4_v49', 'tof_4_v50', 'tof_4_v51', 'tof_4_v52', 'tof_4_v53', 'tof_4_v54', 'tof_4_v55', 'tof_4_v56', 'tof_4_v57', 'tof_4_v58', 'tof_4_v59', 'tof_4_v60', 'tof_4_v61', 'tof_4_v62', 'tof_4_v63', 'tof_5_v0', 'tof_5_v1', 'tof_5_v2', 'tof_5_v3', 'tof_5_v4', 'tof_5_v5', 'tof_5_v6', 'tof_5_v7', 'tof_5_v8', 'tof_5_v9', 'tof_5_v10', 'tof_5_v11', 'tof_5_v12', 'tof_5_v13', 'tof_5_v14', 'tof_5_v15', 'tof_5_v16', 'tof_5_v17', 'tof_5_v18', 'tof_5_v19', 'tof_5_v20', 'tof_5_v21', 'tof_5_v22', 'tof_5_v23', 'tof_5_v24', 'tof_5_v25', 'tof_5_v26', 'tof_5_v27', 'tof_5_v28', 'tof_5_v29', 'tof_5_v30', 'tof_5_v31', 'tof_5_v32', 'tof_5_v33', 'tof_5_v34', 'tof_5_v35', 'tof_5_v36', 'tof_5_v37', 'tof_5_v38', 'tof_5_v39', 'tof_5_v40', 'tof_5_v41', 'tof_5_v42', 'tof_5_v43', 'tof_5_v44', 'tof_5_v45', 'tof_5_v46', 'tof_5_v47', 'tof_5_v48', 'tof_5_v49', 'tof_5_v50', 'tof_5_v51', 'tof_5_v52', 'tof_5_v53', 'tof_5_v54', 'tof_5_v55', 'tof_5_v56', 'tof_5_v57', 'tof_5_v58', 'tof_5_v59', 'tof_5_v60', 'tof_5_v61', 'tof_5_v62', 'tof_5_v63']\n"
     ]
    }
   ],
   "source": [
    "print('='*80)\n",
    "print('COLUMN COMPARISON')\n",
    "print('='*80)\n",
    "\n",
    "print(f'\\nTrain columns: {len(train_df.columns)}')\n",
    "print(f'Test columns: {len(test_df.columns)}')\n",
    "\n",
    "# Find columns present in train but not in test\n",
    "missing_in_test = set(train_df.columns) - set(test_df.columns)\n",
    "print(f'\\nColumns in TRAIN but NOT in TEST: {missing_in_test}')\n",
    "\n",
    "# Find common columns\n",
    "common_cols = set(train_df.columns) & set(test_df.columns)\n",
    "print(f'Common columns: {len(common_cols)}')\n",
    "\n",
    "# Print all test columns\n",
    "print('\\nAll TEST columns:')\n",
    "print(test_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5af65a2",
   "metadata": {},
   "source": [
    "# Feature selection and splitting\n",
    "\n",
    "Define the label column and select feature columns by excluding metadata columns. Create `X_train`, `y_train` and align `X_test` with available test features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3307f2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns: 335\n",
      "Label column: sequence_type\n",
      "\n",
      "X_train shape: (574945, 335)\n",
      "y_train shape: (574945,)\n",
      "X_test shape: (107, 332)\n",
      "\n",
      "Columns in train but not test: {'behavior', 'phase', 'gesture'}\n"
     ]
    }
   ],
   "source": [
    "# Label column\n",
    "label_col = 'sequence_type'\n",
    "\n",
    "# Feature columns (exclude metadata)\n",
    "metadata_cols = ['row_id', 'sequence_id', 'sequence_counter', 'subject', \n",
    "                 'orientation', label_col]\n",
    "feature_cols = [col for col in train_df.columns if col not in metadata_cols]\n",
    "\n",
    "print(f'Feature columns: {len(feature_cols)}')\n",
    "print(f'Label column: {label_col}')\n",
    "\n",
    "# Split features and label\n",
    "X_train = train_df[feature_cols].copy()\n",
    "y_train = train_df[label_col].copy()\n",
    "\n",
    "# Important: for test_df, only select columns that exist in test_df\n",
    "feature_cols_test = [col for col in feature_cols if col in test_df.columns]\n",
    "\n",
    "X_test = test_df[feature_cols_test].copy()\n",
    "\n",
    "print(f'\\nX_train shape: {X_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "\n",
    "print(f'\\nColumns in train but not test: {set(feature_cols) - set(feature_cols_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1903ed4",
   "metadata": {},
   "source": [
    "# Missing value handling\n",
    "\n",
    "Inspect missing values and apply imputation:\n",
    "- Drop columns with >50% missing values.\n",
    "- Fill numeric columns using the training median.\n",
    "- Fill categorical/string columns using the training mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45163b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HANDLING MISSING VALUES\n",
      "================================================================================\n",
      "\n",
      "Missing values in X_train:\n",
      "           Missing_Count  Missing_Percentage\n",
      "thm_5              33286            5.789423\n",
      "tof_5_v63          30142            5.242588\n",
      "tof_5_v24          30142            5.242588\n",
      "tof_5_v18          30142            5.242588\n",
      "tof_5_v19          30142            5.242588\n",
      "tof_5_v20          30142            5.242588\n",
      "tof_5_v21          30142            5.242588\n",
      "tof_5_v22          30142            5.242588\n",
      "tof_5_v23          30142            5.242588\n",
      "tof_5_v25          30142            5.242588\n",
      "tof_5_v16          30142            5.242588\n",
      "tof_5_v26          30142            5.242588\n",
      "tof_5_v27          30142            5.242588\n",
      "tof_5_v28          30142            5.242588\n",
      "tof_5_v29          30142            5.242588\n",
      "tof_5_v30          30142            5.242588\n",
      "tof_5_v31          30142            5.242588\n",
      "tof_5_v17          30142            5.242588\n",
      "tof_5_v15          30142            5.242588\n",
      "tof_5_v33          30142            5.242588\n",
      "\n",
      "Columns to drop (>50% missing): 0\n",
      "\n",
      "Numeric columns: 332\n",
      "String columns: 3\n",
      "String column examples: ['behavior', 'phase', 'gesture']\n",
      "\n",
      "After handling missing values:\n",
      "X_train shape: (574945, 335)\n",
      "X_test shape: (107, 332)\n",
      "Remaining missing values in X_train: 0\n",
      "Remaining missing values in X_test: 0\n"
     ]
    }
   ],
   "source": [
    "print('='*80)\n",
    "print('HANDLING MISSING VALUES')\n",
    "print('='*80)\n",
    "\n",
    "# Check missing values (only common columns)\n",
    "common_feature_cols = [col for col in X_train.columns if col in X_test.columns]\n",
    "\n",
    "print(f'\\nMissing values in X_train:')\n",
    "missing_counts = X_train[common_feature_cols].isnull().sum()\n",
    "missing_pct = (missing_counts / len(X_train)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_counts,\n",
    "    'Missing_Percentage': missing_pct\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "print(missing_df.head(20))\n",
    "\n",
    "# Strategy: drop columns with >50% missing values\n",
    "cols_to_drop = missing_df[missing_df['Missing_Percentage'] > 50].index.tolist()\n",
    "print(f'\\nColumns to drop (>50% missing): {len(cols_to_drop)}')\n",
    "\n",
    "# Drop these columns\n",
    "X_train = X_train.drop(columns=cols_to_drop)\n",
    "X_test = X_test.drop(columns=[col for col in cols_to_drop if col in X_test.columns])\n",
    "\n",
    "# Fill remaining missing values: numeric -> median, string/object -> mode\n",
    "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "string_cols = X_train.select_dtypes(include=['str', 'object']).columns.tolist()\n",
    "\n",
    "print(f'\\nNumeric columns: {len(numeric_cols)}')\n",
    "print(f'String columns: {len(string_cols)}')\n",
    "if string_cols:\n",
    "    print(f'String column examples: {string_cols[:5]}')\n",
    "\n",
    "# Compute medians on training numeric columns\n",
    "train_medians = X_train[numeric_cols].median()\n",
    "\n",
    "# Fill numeric columns with medians\n",
    "X_train[numeric_cols] = X_train[numeric_cols].fillna(train_medians)\n",
    "for col in numeric_cols:\n",
    "    if col in X_test.columns:\n",
    "        X_test[col] = X_test[col].fillna(train_medians[col])\n",
    "\n",
    "# Fill string/object columns with mode\n",
    "for col in string_cols:\n",
    "    mode_value = X_train[col].mode()[0]\n",
    "    X_train[col] = X_train[col].fillna(mode_value)\n",
    "    if col in X_test.columns:\n",
    "        X_test[col] = X_test[col].fillna(mode_value)\n",
    "\n",
    "print(f'\\nAfter handling missing values:')\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "print(f'Remaining missing values in X_train: {X_train.isnull().sum().sum()}')\n",
    "print(f'Remaining missing values in X_test: {X_test.isnull().sum().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baa4168",
   "metadata": {},
   "source": [
    "# Encode target variable\n",
    "\n",
    "Map categorical target labels to numeric values for modeling.\n",
    "- Example mapping: `Target` -> 1, `Non-Target` -> 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d16831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ENCODING TARGET VARIABLE\n",
      "================================================================================\n",
      "\n",
      "Original label distribution:\n",
      "sequence_type\n",
      "Target        344058\n",
      "Non-Target    230887\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Encoded label distribution:\n",
      "sequence_type\n",
      "1    344058\n",
      "0    230887\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label mapping: {'Target': 1, 'Non-Target': 0}\n"
     ]
    }
   ],
   "source": [
    "print('='*80)\n",
    "print('ENCODING TARGET VARIABLE')\n",
    "print('='*80)\n",
    "\n",
    "# View label distribution\n",
    "print('\\nOriginal label distribution:')\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Encode: Target=1, Non-Target=0\n",
    "label_mapping = {'Target': 1, 'Non-Target': 0}\n",
    "y_train_encoded = y_train.map(label_mapping)\n",
    "\n",
    "print('\\nEncoded label distribution:')\n",
    "print(y_train_encoded.value_counts())\n",
    "print(f'\\nLabel mapping: {label_mapping}')\n",
    "\n",
    "y_train = y_train_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da88a25",
   "metadata": {},
   "source": [
    "# Train / Validation split\n",
    "\n",
    "Split the processed training data into training and validation sets (80/20) while preserving class proportions using stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afef12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAIN-VALIDATION SPLIT\n",
      "================================================================================\n",
      "\n",
      "Train split: (459956, 335)\n",
      "Validation split: (114989, 335)\n",
      "Test: (107, 332)\n",
      "\n",
      "Class distribution in train split:\n",
      "sequence_type\n",
      "1    275246\n",
      "0    184710\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution in validation split:\n",
      "sequence_type\n",
      "1    68812\n",
      "0    46177\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('='*80)\n",
    "print('TRAIN-VALIDATION SPLIT')\n",
    "print('='*80)\n",
    "\n",
    "# 80-20 split\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_train  # preserve class proportions\n",
    ")\n",
    "\n",
    "print(f'\\nTrain split: {X_train_split.shape}')\n",
    "print(f'Validation split: {X_val.shape}')\n",
    "print(f'Test: {X_test.shape}')\n",
    "\n",
    "print(f'\\nClass distribution in train split:')\n",
    "print(y_train_split.value_counts())\n",
    "print(f'\\nClass distribution in validation split:')\n",
    "print(y_val.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4758654d",
   "metadata": {},
   "source": [
    "# Remove non-numeric columns\n",
    "\n",
    "Detect and remove `object`/string columns from each split to ensure only numeric data is scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec53cfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for string columns...\n",
      "\n",
      "Final shapes:\n",
      "X_train_split: (459956, 332) - dtypes: {dtype('float64'): 332}\n",
      "X_val: (114989, 332) - dtypes: {dtype('float64'): 332}\n",
      "X_test: (107, 332) - dtypes: {dtype('float64'): 332}\n",
      "\n",
      "✅ Ready for scaling!\n"
     ]
    }
   ],
   "source": [
    "# Ensure removal of any string/object columns\n",
    "print('Checking for string columns...')\n",
    "\n",
    "# Remove string/object columns from X_train_split\n",
    "string_cols_train = X_train_split.select_dtypes(include=['str','object']).columns.tolist()\n",
    "if string_cols_train:\n",
    "    print(f'Removing {len(string_cols_train)} string columns from X_train_split: {string_cols_train}')\n",
    "    X_train_split = X_train_split.drop(columns=string_cols_train)\n",
    "\n",
    "# Remove string/object columns from X_val\n",
    "string_cols_val = X_val.select_dtypes(include=['str','object']).columns.tolist()\n",
    "if string_cols_val:\n",
    "    print(f'Removing {len(string_cols_val)} string columns from X_val: {string_cols_val}')\n",
    "    X_val = X_val.drop(columns=string_cols_val)\n",
    "\n",
    "# Remove string/object columns from X_test\n",
    "string_cols_test = X_test.select_dtypes(include=['str','object']).columns.tolist()\n",
    "if string_cols_test:\n",
    "    print(f'Removing {len(string_cols_test)} string columns from X_test: {string_cols_test}')\n",
    "    X_test = X_test.drop(columns=string_cols_test)\n",
    "\n",
    "print(f'\\nFinal shapes:')\n",
    "print(f'X_train_split: {X_train_split.shape} - dtypes: {X_train_split.dtypes.value_counts().to_dict()}')\n",
    "print(f'X_val: {X_val.shape} - dtypes: {X_val.dtypes.value_counts().to_dict()}')\n",
    "print(f'X_test: {X_test.shape} - dtypes: {X_test.dtypes.value_counts().to_dict()}')\n",
    "\n",
    "print('\\n Ready for scaling!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f385a734",
   "metadata": {},
   "source": [
    "# Standardization\n",
    "\n",
    "Fit a `StandardScaler` on the training split and apply it to validation and test sets. Convert results back to DataFrames for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15ddd41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STANDARDIZATION\n",
      "================================================================================\n",
      "Scaler fitted on train split\n",
      "Mean of scaled train: 0.000000\n",
      "Std of scaled train: 1.000000\n",
      "\n",
      "Scaled shapes:\n",
      "X_train_scaled: (459956, 332)\n",
      "X_val_scaled: (114989, 332)\n",
      "X_test_scaled: (107, 332)\n"
     ]
    }
   ],
   "source": [
    "print('='*80)\n",
    "print('STANDARDIZATION')\n",
    "print('='*80)\n",
    "\n",
    "# Fit scaler on train split\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_split)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print('Scaler fitted on train split')\n",
    "print(f'Mean of scaled train: {X_train_scaled.mean():.6f}')\n",
    "print(f'Std of scaled train: {X_train_scaled.std():.6f}')\n",
    "\n",
    "# Convert back to DataFrame\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train_split.columns)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "print('\\nScaled shapes:')\n",
    "print(f'X_train_scaled: {X_train_scaled.shape}')\n",
    "print(f'X_val_scaled: {X_val_scaled.shape}')\n",
    "print(f'X_test_scaled: {X_test_scaled.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d79944",
   "metadata": {},
   "source": [
    "# Save preprocessed data\n",
    "\n",
    "Persist the preprocessed datasets and the fitted scaler to `../data/` using pickle for downstream modeling and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aab1b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAVING PREPROCESSED DATA\n",
      "================================================================================\n",
      "✅ Preprocessed data saved:\n",
      "  - X_train_preprocessed.pkl\n",
      "  - X_val_preprocessed.pkl\n",
      "  - X_test_preprocessed.pkl\n",
      "  - y_train_preprocessed.pkl\n",
      "  - y_val_preprocessed.pkl\n",
      "  - scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "print('='*80)\n",
    "print('SAVING PREPROCESSED DATA')\n",
    "print('='*80)\n",
    "\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "# Save as pickle (preserve column info)\n",
    "with open('../data/X_train_preprocessed.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train_scaled, f)\n",
    "\n",
    "with open('../data/X_val_preprocessed.pkl', 'wb') as f:\n",
    "    pickle.dump(X_val_scaled, f)\n",
    "\n",
    "with open('../data/X_test_preprocessed.pkl', 'wb') as f:\n",
    "    pickle.dump(X_test_scaled, f)\n",
    "\n",
    "with open('../data/y_train_preprocessed.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train_split, f)\n",
    "\n",
    "with open('../data/y_val_preprocessed.pkl', 'wb') as f:\n",
    "    pickle.dump(y_val, f)\n",
    "\n",
    "# Also save the scaler (for later inference)\n",
    "with open('../data/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print('  Preprocessed data saved:')\n",
    "print('  - X_train_preprocessed.pkl')\n",
    "print('  - X_val_preprocessed.pkl')\n",
    "print('  - X_test_preprocessed.pkl')\n",
    "print('  - y_train_preprocessed.pkl')\n",
    "print('  - y_val_preprocessed.pkl')\n",
    "print('  - scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a3dea7",
   "metadata": {},
   "source": [
    "# Preprocessing summary\n",
    "\n",
    "Summarize preprocessing results (rows, dropped columns, final feature count, split sizes, and class counts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61659544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREPROCESSING SUMMARY\n",
      "================================================================================\n",
      "Original Train Rows: 574945\n",
      "Columns Dropped: 0\n",
      "Final Features: 332\n",
      "Train Split: 459956\n",
      "Validation Split: 114989\n",
      "Test: 107\n",
      "Target=1: 275246\n",
      "Target=0: 184710\n",
      "\n",
      "✅ Preprocessing Complete!\n"
     ]
    }
   ],
   "source": [
    "print('='*80)\n",
    "print('PREPROCESSING SUMMARY')\n",
    "print('='*80)\n",
    "\n",
    "summary = {\n",
    "    'Original Train Rows': len(train_df),\n",
    "    'Columns Dropped': len(cols_to_drop),\n",
    "    'Final Features': X_train_scaled.shape[1],\n",
    "    'Train Split': X_train_scaled.shape[0],\n",
    "    'Validation Split': X_val_scaled.shape[0],\n",
    "    'Test': X_test_scaled.shape[0],\n",
    "    'Target=1': (y_train_split == 1).sum(),\n",
    "    'Target=0': (y_train_split == 0).sum(),\n",
    "}\n",
    "\n",
    "for key, value in summary.items():\n",
    "    print(f'{key}: {value}')\n",
    "\n",
    "print('\\n Preprocessing Complete!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
